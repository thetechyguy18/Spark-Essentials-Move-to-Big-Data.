{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fefa779-0b83-49b1-9724-3f755b414a88",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4796d33-3049-42b6-9a5f-c89c05ae25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\deepe\\OneDrive\\Desktop\\Python_spark\\DB_managed\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9f5a1-605c-4e03-986e-4e9fac96438a",
   "metadata": {},
   "source": [
    "## Creating Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcbaf42-eda7-4ae6-966b-bb5400be765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE SCHOOLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bea214-1ebd-424b-b698-ab81140683c3",
   "metadata": {},
   "source": [
    "## Checking if we created or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "833e0fb9-b09c-41bc-9ccb-4161ab62a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  schools|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SHOW DATABASES\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9321dda-4f94-4576-aeb3-fece8fb987b1",
   "metadata": {},
   "source": [
    "## Creating Table in school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0817244d-929b-4a16-aa1f-baee8151034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE SCHOOLS.STUDENTS (\n",
    "    ID INT,\n",
    "    NAME STRING,\n",
    "    CLASS STRING\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4cfa5-f3ed-45dd-9cfe-578c3e42b19d",
   "metadata": {},
   "source": [
    "## Adding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582fb300-ee9d-470d-b133-d937747b5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' INSERT INTO SCHOOLS.STUDENTS VALUES\n",
    "(1,'RAM','10th'),\n",
    "(2,'SHAM','10th'),\n",
    "(3,'AMAN','10th')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8c255-7282-4f53-a52b-beaf4e685a5a",
   "metadata": {},
   "source": [
    "## Checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdce2ab5-943c-4f57-8aae-d487bdfd099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| ID|NAME|CLASS|\n",
      "+---+----+-----+\n",
      "|  2|SHAM| 10th|\n",
      "|  3|AMAN| 10th|\n",
      "|  1| RAM| 10th|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' select * from SCHOOLS.STUDENTS\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e89990-15d7-4e1f-82e3-67c2c48478e3",
   "metadata": {},
   "source": [
    "## Checking the table type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b15cce-3e52-4339-841b-bbecf32b82ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                        |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|ID                          |int                                                                              |NULL   |\n",
      "|NAME                        |string                                                                           |NULL   |\n",
      "|CLASS                       |string                                                                           |NULL   |\n",
      "|                            |                                                                                 |       |\n",
      "|# Detailed Table Information|                                                                                 |       |\n",
      "|Catalog                     |spark_catalog                                                                    |       |\n",
      "|Database                    |schools                                                                          |       |\n",
      "|Table                       |students                                                                         |       |\n",
      "|Owner                       |deepe                                                                            |       |\n",
      "|Created Time                |Fri Jan 23 11:46:59 IST 2026                                                     |       |\n",
      "|Last Access                 |UNKNOWN                                                                          |       |\n",
      "|Created By                  |Spark 4.0.1                                                                      |       |\n",
      "|Type                        |MANAGED                                                                          |       |\n",
      "|Provider                    |parquet                                                                          |       |\n",
      "|Location                    |file:/C:/Users/deepe/OneDrive/Desktop/Python_spark/DB_managed/schools.db/students|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                    |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                   |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE FORMATTED SCHOOLS.STUDENTS\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfd9a9-f284-4090-b1f7-68197f3ffdb3",
   "metadata": {},
   "source": [
    "## Creating External Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24452f90-ed7f-4c4e-a194-46ac28e6d640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(r\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS TEACHERS_EXT (\n",
    "    ID INT,\n",
    "    NAME STRING,\n",
    "    SUBJECT STRING\n",
    ")\n",
    "USING PARQUET\n",
    "LOCATION 'C:/Users/deepe/OneDrive/Desktop/Python_spark/DB_External'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b9eb0e-6f7e-4de0-a1b9-e126a5fe685d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql('drop table TEACHERS_EXT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4f944-e253-42d6-8aaf-9ad8700bf489",
   "metadata": {},
   "source": [
    "## Adding data in the table above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb07a387-992d-45a4-b482-c248035bd371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO TEACHERS_EXT VALUES\n",
    "(1, 'Ravi', 'Math'),\n",
    "(2, 'Anita', 'Science'),\n",
    "(3, 'John', 'English')\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee3a65c1-9cc1-436f-b43b-d5b5201a6d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|teachers_ext|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd397c-c3e7-4840-a373-ced0152098d0",
   "metadata": {},
   "source": [
    "## Checking the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7081bfc2-66f3-4962-be5c-f150bbcdffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| ID| NAME|SUBJECT|\n",
      "+---+-----+-------+\n",
      "|  2|Anita|Science|\n",
      "|  2|Anita|Science|\n",
      "|  3| John|English|\n",
      "|  3| John|English|\n",
      "|  1| Ravi|   Math|\n",
      "|  1| Ravi|   Math|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' select * from teachers_ext\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca46347-b1e7-48e7-96ba-4ee804ff18eb",
   "metadata": {},
   "source": [
    "## Checking the table type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66da68dc-c230-41bf-87a0-c7b8d6716fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                     |comment|\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "|ID                          |int                                                           |NULL   |\n",
      "|NAME                        |string                                                        |NULL   |\n",
      "|SUBJECT                     |string                                                        |NULL   |\n",
      "|                            |                                                              |       |\n",
      "|# Detailed Table Information|                                                              |       |\n",
      "|Catalog                     |spark_catalog                                                 |       |\n",
      "|Database                    |default                                                       |       |\n",
      "|Table                       |teachers_ext                                                  |       |\n",
      "|Owner                       |deepe                                                         |       |\n",
      "|Created Time                |Fri Jan 23 11:53:44 IST 2026                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                       |       |\n",
      "|Created By                  |Spark 4.0.1                                                   |       |\n",
      "|Type                        |EXTERNAL                                                      |       |\n",
      "|Provider                    |PARQUET                                                       |       |\n",
      "|Location                    |file:/C:/Users/deepe/OneDrive/Desktop/Python_spark/DB_External|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n",
      "+----------------------------+--------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE FORMATTED TEACHERS_EXT\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "452eb493-616b-4985-96ba-e91de3d36ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "drop table schools.students\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad15368d-8237-4032-8216-8dbd410c38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select * from  schools.students\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "232b8f24-1cd0-44b0-85df-a6affaa6e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop database schools')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946d7ef-ea28-4865-955d-986514da7408",
   "metadata": {},
   "source": [
    "## Deleting the External table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e38b41a-17da-416b-8db6-e6ce49b53ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "drop table  TEACHERS_EXT\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84cc73c5-0e28-443b-a5d0-ad2b52f869db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-23 12:05:06.461\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `TEACHERS_EXT` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o32.sql.\\n: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `TEACHERS_EXT` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 2 pos 15;\\n'Project [*]\\n+- 'UnresolvedRelation [TEACHERS_EXT], [], false\\n\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\r\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\r\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:91)\\r\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\r\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\r\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\r\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\r\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\r\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\r\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\r\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\r\\n\\tat java.base/java.lang.Thread.run(Thread.java:842)\\r\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\r\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\r\\n\\t\\t... 22 more\\r\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"D:\\\\spark_setup_folder\\\\spark\\\\python\\\\pyspark\\\\errors\\\\exceptions\\\\captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"D:\\\\spark_setup_folder\\\\spark\\\\python\\\\lib\\\\py4j-0.10.9.9-src.zip\\\\py4j\\\\protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `TEACHERS_EXT` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 2 pos 15;\n'Project [*]\n+- 'UnresolvedRelation [TEACHERS_EXT], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43mSELECT * FROM  TEACHERS_EXT\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\sql\\session.py:1810\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1806\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1807\u001b[0m             errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1808\u001b[0m             messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[1;32m-> 1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `TEACHERS_EXT` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 2 pos 15;\n'Project [*]\n+- 'UnresolvedRelation [TEACHERS_EXT], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM  TEACHERS_EXT\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f7982-5b8c-4d5d-ada8-d434b0f21f89",
   "metadata": {},
   "source": [
    "## Reading predefined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46667c4a-fff1-415d-8601-bd76e11dbd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(r\"\"\"\n",
    "CREATE TABLE students_ext_dir\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "    path r\"C:\\Users\\deepe\\OneDrive\\Desktop\\Python_spark\\Dataset\\students.csv\",\n",
    "    header 'true',\n",
    "    inferSchema 'true'\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85d4cab9-dc2c-4813-9994-29b86be41b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|class|\n",
      "+---+-----+-----+\n",
      "|  1| Ravi|   10|\n",
      "|  2|Anita|    9|\n",
      "|  3| John|   10|\n",
      "|  4|Priya|    8|\n",
      "|  5| Aman|    9|\n",
      "|  6|Sneha|   10|\n",
      "|  7|Karan|    8|\n",
      "|  8| Neha|    9|\n",
      "|  9|Vikas|   10|\n",
      "| 10|Pooja|    8|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(' select * from students_ext_dir').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cd61e9f-1bdd-4902-80a3-6bbb587d9a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                              |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                    |NULL   |\n",
      "|name                        |string                                                                 |NULL   |\n",
      "|class                       |int                                                                    |NULL   |\n",
      "|                            |                                                                       |       |\n",
      "|# Detailed Table Information|                                                                       |       |\n",
      "|Catalog                     |spark_catalog                                                          |       |\n",
      "|Database                    |default                                                                |       |\n",
      "|Table                       |students_ext_dir                                                       |       |\n",
      "|Owner                       |deepe                                                                  |       |\n",
      "|Created Time                |Fri Jan 23 12:07:59 IST 2026                                           |       |\n",
      "|Last Access                 |UNKNOWN                                                                |       |\n",
      "|Created By                  |Spark 4.0.1                                                            |       |\n",
      "|Type                        |EXTERNAL                                                               |       |\n",
      "|Provider                    |CSV                                                                    |       |\n",
      "|Location                    |file:/C:/Users/deepe/OneDrive/Desktop/Python_spark/Dataset/students.csv|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                     |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                       |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat              |       |\n",
      "|Storage Properties          |[header=true, inferSchema=true]                                        |       |\n",
      "+----------------------------+-----------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended students_ext_dir').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e5d7692-c02b-45dc-86cf-494990420faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('drop table students_ext_dir').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15dae4a7-4534-4007-99db-e2321851f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-23 12:11:27.216\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[TABLE_OR_VIEW_NOT_FOUND] The table or view `students_ext_dir` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01\", \"context\": {\"errorClass\": \"TABLE_OR_VIEW_NOT_FOUND\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o32.sql.\\n: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `students_ext_dir` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 15;\\n'Project [*]\\n+- 'UnresolvedRelation [students_ext_dir], [], false\\n\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\r\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\r\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\r\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\r\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\r\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$5(Dataset.scala:139)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:136)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$1(SparkSession.scala:462)\\r\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:449)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:467)\\r\\n\\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:91)\\r\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\\r\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\r\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\\r\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\r\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\r\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\r\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\r\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\r\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\r\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\r\\n\\tat java.base/java.lang.Thread.run(Thread.java:842)\\r\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:91)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:306)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\r\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\r\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\r\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\r\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\r\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\r\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\r\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\r\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\r\\n\\t\\t... 22 more\\r\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"D:\\\\spark_setup_folder\\\\spark\\\\python\\\\pyspark\\\\errors\\\\exceptions\\\\captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"D:\\\\spark_setup_folder\\\\spark\\\\python\\\\lib\\\\py4j-0.10.9.9-src.zip\\\\py4j\\\\protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `students_ext_dir` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 15;\n'Project [*]\n+- 'UnresolvedRelation [students_ext_dir], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselect * from  students_ext_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow() \n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\sql\\session.py:1810\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1806\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1807\u001b[0m             errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVALID_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1808\u001b[0m             messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(args)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1809\u001b[0m         )\n\u001b[1;32m-> 1810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `students_ext_dir` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 15;\n'Project [*]\n+- 'UnresolvedRelation [students_ext_dir], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from  students_ext_dir').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77522bce-b667-45f7-b04b-da6f5f0b068a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Spark)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
