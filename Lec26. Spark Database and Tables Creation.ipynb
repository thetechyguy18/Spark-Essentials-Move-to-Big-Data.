{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edddde94-8d2c-4803-8f17-7192f1279589",
   "metadata": {},
   "source": [
    "## SPARK SESSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3915daaf-d331-419c-95bb-dc73210385f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"SparkTest\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\deepe\\OneDrive\\Desktop\\Python_spark\\DB_details\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "617ace15-2910-441e-8f13-cf185b3b5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78084d86-35ad-48d0-a307-7dea5f1c7373",
   "metadata": {},
   "source": [
    "## creating a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8d34e6c-b7c2-4ae5-8801-912d5ce46600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE IF EXISTS SCHOOL CASCADE\")\n",
    "spark.sql(\"CREATE DATABASE SCHOOL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1803d8-46dc-4af3-bb43-9c0b063b0b77",
   "metadata": {},
   "source": [
    "### shows the databases which we have, default is already present which is created by spark to manage the tempViews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbe62597-0533-4a2d-8c57-f550e135158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   school|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SHOW DATABASES\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ddb2a-bbaa-4599-bc5c-363555b9d120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8de802-0f14-49cd-9e17-900271236af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' SHOW TABLES\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc9763c-d6bf-4dd4-a131-32c30226522e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE SCHOOL.STUDENTS (\n",
    "    ID INT,\n",
    "    NAME STRING,\n",
    "    CLASS STRING\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef4022-cd7b-4c30-b333-3eb7327c1947",
   "metadata": {},
   "source": [
    "## NOW INSERTING DATA IN THE TABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cbeed08-8530-4a5e-9497-f6996f388b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' INSERT INTO SCHOOL.STUDENTS VALUES\n",
    "(1,'RAM','10th'),\n",
    "(2,'SHAM','10th'),\n",
    "(3,'AMAN','10th')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db628a1-9937-440f-8682-18e70dbc4249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| ID|NAME|CLASS|\n",
      "+---+----+-----+\n",
      "|  2|SHAM| 10th|\n",
      "|  3|AMAN| 10th|\n",
      "|  1| RAM| 10th|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' select * from SCHOOL.STUDENTS\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2be62530-2643-4ab2-9a76-9f5bc63bbdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                       |comment|\n",
      "+----------------------------+--------------------------------------------------------------------------------+-------+\n",
      "|ID                          |int                                                                             |NULL   |\n",
      "|NAME                        |string                                                                          |NULL   |\n",
      "|CLASS                       |string                                                                          |NULL   |\n",
      "|                            |                                                                                |       |\n",
      "|# Detailed Table Information|                                                                                |       |\n",
      "|Catalog                     |spark_catalog                                                                   |       |\n",
      "|Database                    |school                                                                          |       |\n",
      "|Table                       |students                                                                        |       |\n",
      "|Owner                       |deepe                                                                           |       |\n",
      "|Created Time                |Thu Jan 22 16:51:18 IST 2026                                                    |       |\n",
      "|Last Access                 |UNKNOWN                                                                         |       |\n",
      "|Created By                  |Spark 4.0.1                                                                     |       |\n",
      "|Type                        |MANAGED                                                                         |       |\n",
      "|Provider                    |parquet                                                                         |       |\n",
      "|Location                    |file:/C:/Users/deepe/OneDrive/Desktop/Python_spark/DB_details/school.db/students|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                     |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                   |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                  |       |\n",
      "+----------------------------+--------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE FORMATTED SCHOOL.STUDENTS\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635e7c1-dad8-49fd-a7d0-ef45b429566f",
   "metadata": {},
   "source": [
    "## RIGHT NOW SPARK IS USING DEFAULT DATABASE SO WE CAN'T SEE ANYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04bb15f8-e230-4271-97f3-043cffcec602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' SHOW TABLES\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfdb98-64bb-47da-ad58-0e406b3e38ce",
   "metadata": {},
   "source": [
    "## First use that database which we created to see tables inside it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4b35c5-f8f5-4d57-af9c-32a4c393dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' USE SCHOOL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f8ec0-3860-4309-9bb5-220cd56797d7",
   "metadata": {},
   "source": [
    "## Now you can see all these are there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a1e251-85dd-40e8-97e1-011472d7669f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|   school| students|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(''' SHOW TABLES\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57adfbdc-2307-4edb-aaa4-6673fa38397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "CREATE OR REPLACE TEMP VIEW TEACHER AS\n",
    "SELECT * FROM VALUES\n",
    "    (1, 'Ramesh', 'Maths'),\n",
    "    (2, 'Suresh', 'Science'),\n",
    "    (3, 'Anita', 'English')\n",
    "AS TEACHER(ID, NAME, SUBJECT)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a3bd39-9a7c-4fa3-82ef-0b1ad8aceabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7070c999-5a0f-4de6-a2dc-8676cb30b048",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o108.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\r\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:93)\r\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:76)\r\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:116)\r\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:175)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:175)\r\n\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:174)\r\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:186)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:184)\r\n\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:181)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:197)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:197)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:91)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\sql\\session.py:681\u001b[0m, in \u001b[0;36mSparkSession._repr_html_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124m        <div>\u001b[39m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[38;5;124m</b></p>\u001b[39m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124m        </div>\u001b[39m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m--> 681\u001b[0m         catalogImplementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalogImplementation\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    682\u001b[0m         sc_HTML\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39m_repr_html_(),\n\u001b[0;32m    683\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\sql\\session.py:853\u001b[0m, in \u001b[0;36mSparkSession.conf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconf\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RuntimeConfig:\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runtime configuration interface for Spark.\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \n\u001b[0;32m    829\u001b[0m \u001b[38;5;124;03m    This is the interface through which the user can get and set all Spark and Hadoop\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m    'value'\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RuntimeConfig(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\spark_setup_folder\\spark\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o108.conf.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\r\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:93)\r\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:76)\r\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:116)\r\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sharedState$1(SparkSession.scala:175)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.classic.SparkSession.sharedState$lzycompute(SparkSession.scala:175)\r\n\tat org.apache.spark.sql.classic.SparkSession.sharedState(SparkSession.scala:174)\r\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sessionState$2(SparkSession.scala:186)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.classic.SparkSession.sessionState$lzycompute(SparkSession.scala:184)\r\n\tat org.apache.spark.sql.classic.SparkSession.sessionState(SparkSession.scala:181)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf$lzycompute(SparkSession.scala:197)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:197)\r\n\tat org.apache.spark.sql.classic.SparkSession.conf(SparkSession.scala:91)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x172a57f32b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6c396-0a71-447e-98d0-bc1f82d9a483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Spark)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
